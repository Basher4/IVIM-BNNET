{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e669fe16-223a-4f51-b02c-a1bf29a04a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./sw/IVIMNET/deep.py:882: UserWarning: arg no norm_data_full. Using default of False\n",
      "  warnings.warn('arg no norm_data_full. Using default of False')\n",
      "./sw/IVIMNET/deep.py:789: UserWarning: arg.train.plateau_size not defined. Using default of 20\n",
      "  warnings.warn('arg.train.plateau_size not defined. Using default of 20')\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "sys.path.append('./')\n",
    "sys.path.append('./sw/')\n",
    "\n",
    "from IVIMNET.hyperparams import hyperparams as hp_example_1\n",
    "import IVIMNET.simulations as sim\n",
    "import IVIMNET.deep as deep\n",
    "import IVIMNET.deep_bayes as deep_bayes\n",
    "import IVIMNET.fitting_algorithms as fit\n",
    "\n",
    "arg = hp_example_1()\n",
    "arg = deep.checkarg(arg)\n",
    "arg.fit.do_fit = False\n",
    "\n",
    "testdata = False\n",
    "\n",
    "EPOCHS = 1\n",
    "BAYES_SAMPLES = 32\n",
    "\n",
    "### folder patient data\n",
    "folder = './sw/data/example_data'\n",
    "model_path = './sw/models/bnn_e09_ivim_patient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e7288c-c71b-4375-9965-a75daf32c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load patient data \n",
      "\n",
      "Patient data loaded\n",
      "\n",
      "NN fitting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### load patient data\n",
    "print('Load patient data \\n')\n",
    "# load and init b-values\n",
    "text_file = np.genfromtxt('{folder}/bvalues.bval'.format(folder=folder))\n",
    "bvalues = np.array(text_file)\n",
    "selsb = np.array(bvalues) == 0\n",
    "# load nifti\n",
    "data = nib.load('{folder}/data.nii.gz'.format(folder=folder))\n",
    "datas = data.get_fdata() \n",
    "# reshape image for fitting\n",
    "sx, sy, sz, n_b_values = datas.shape \n",
    "X_dw = np.reshape(datas, (sx * sy * sz, n_b_values))\n",
    "\n",
    "### select only relevant values, delete background and noise, and normalise data\n",
    "S0 = np.nanmean(X_dw[:, selsb], axis=1)\n",
    "S0[S0 != S0] = 0\n",
    "S0 = np.squeeze(S0)\n",
    "valid_id = (S0 > (0.5 * np.median(S0[S0 > 0]))) \n",
    "datatot = X_dw[valid_id, :]\n",
    "# normalise data\n",
    "S0 = np.nanmean(datatot[:, selsb], axis=1).astype('<f')\n",
    "datatot = datatot / S0[:, None]\n",
    "print('Patient data loaded\\n')\n",
    "\n",
    "print('NN fitting\\n')\n",
    "res = [i for i, val in enumerate(datatot != datatot) if not val.any()] # Remove NaN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b65e981-6674-4df8-a371-093b965430bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./sw/IVIMNET/deep.py:882: UserWarning: arg no norm_data_full. Using default of False\n",
      "  warnings.warn('arg no norm_data_full. Using default of False')\n",
      "./sw/IVIMNET/deep.py:789: UserWarning: arg.train.plateau_size not defined. Using default of 20\n",
      "  warnings.warn('arg.train.plateau_size not defined. Using default of 20')\n"
     ]
    }
   ],
   "source": [
    "arg = hp_example_1()\n",
    "arg = deep.checkarg(arg)\n",
    "arg.fit.do_fit = False\n",
    "arg.train_pars.batch_size = 32\n",
    "net_params = deep_bayes.net_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11b072d-2992-4353-af6b-0fdf1946a947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Epoch: 0; Bad epochs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [00:14, 35.33it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 47.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.050373968482017514\n",
      "############### Saving good model ###############################\n",
      "\n",
      "Done\n",
      "\n",
      "time elapsed for Net: 18.313867077231407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.perf_counter()\n",
    "net, epochs, best_val_loss = deep_bayes.learn_IVIM(datatot[res], bvalues, arg, net_params=net_params, epochs=EPOCHS, stats_out=True, bayes_samples=BAYES_SAMPLES)\n",
    "time_end = time.perf_counter() - time_start\n",
    "print('\\ntime elapsed for Net: {}\\n'.format(time_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ea5897-fdab-41df-8279-b847c9954270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=104, out_features=1, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=104, out_features=1, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=104, out_features=1, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (1): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=104, out_features=104, bias=True)\n",
       "      (5): BernoulliDropout(\n",
       "        p=0.10000000149011612, quant=False\n",
       "        (mul_mask): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (mul_scalar): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): BatchNorm1d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=104, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e748f1a9-2326-42ea-8686-74cc977ae16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110589, 104)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3193bb-5446-47a8-bc88-966a1aae46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86938d1e-198c-465f-b4d6-5a61e47d9ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.37 GiB (GPU 0; 10.92 GiB total capacity; 132.00 MiB already allocated; 1.24 GiB free; 134.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-51dce920823e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minfer_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpu_net(infer_data)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/ccnas2/tdp/mg918/py3-venv/lib64/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ccnas2/tdp/mg918/py3-venv/lib64/python3.6/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ccnas2/tdp/mg918/py3-venv/lib64/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ccnas2/tdp/mg918/py3-venv/lib64/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ccnas2/tdp/mg918/py3-venv/lib64/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m/mnt/ccnas2/tdp/mg918/py3-venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fyp/IVIM-BNNET/sw/IVIMNET/deep_bayes.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mX_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayes_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mparamsSamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayes_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mX_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.37 GiB (GPU 0; 10.92 GiB total capacity; 132.00 MiB already allocated; 1.24 GiB free; 134.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "infer_data = torch.from_numpy(datatot.astype(np.float32)).to(\"cuda\")\n",
    "%timeit cpu_net(infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e76a8-21d6-483f-a315-66a188f11d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
