"""
This file loads parameters of NN generated by 01.py and
    1) Fuses batch normalization parameter with preceeding weights
    2) Finds the best nuber of integer and fractional bits to represent weights and biases
"""

from fixedpoint import FixedPoint
import torch
import sys
import json

sys.path.append('./sw/')

NEURAL_NETWORK_PATH = "./sw/models/bnn_e01_SNR5_0.pt"
ACCELERATOR_NUM_LANES = 32
accelerator_perceptrons = [[]] * ACCELERATOR_NUM_LANES
TOTAL_BITS = 16

# Load neural network
net: torch.nn.Module = torch.load(NEURAL_NETWORK_PATH, map_location="cpu")
network_parameters = dict(net.named_parameters())

# Merge weights with Batch Normalization
def merge_weights_with_bn(lin_w, lin_b, bn_w, bn_b):
    w = lin_w * bn_w
    b = bn_w * lin_b + bn_b
    return w, b

for param_net in range(4):
    for offset in [0, 4]:
        linear_layer: torch.nn.Linear = net.get_submodule(f"fc_layers.{param_net}.{offset}")
        batch_norm: torch.nn.BatchNorm1d = net.get_submodule(f"fc_layers.{param_net}.{offset+2}")

        lin_w, lin_b = linear_layer.weight, linear_layer.bias
        bn_w, bn_b = batch_norm.weight, batch_norm.bias
        for perc_idx in range(lin_w.shape[0]):
            w, b = merge_weights_with_bn(lin_w[perc_idx], lin_b[perc_idx], bn_w, bn_b)
            accelerator_perceptrons[perc_idx % ACCELERATOR_NUM_LANES].append((w,b))

    encoder_layer: torch.nn.Linear = net.get_submodule(f"encoder.{param_net}.8")
    for i in range(ACCELERATOR_NUM_LANES):
        accelerator_perceptrons[i].append((encoder_layer.weight.squeeze(), encoder_layer.bias))

# Find the best quantization
print(f"Every perceptron ({ACCELERATOR_NUM_LANES} in total) requires memory {len(accelerator_perceptrons[0])} deep")

best_int_bits = 0
best_rmse = 9999
for int_bits in range(1, TOTAL_BITS):
    error = 0
    rmse = 0
    denom = 0

    for perc in accelerator_perceptrons:
        for w,b in perc:
            for we in w:
                err = we - float(FixedPoint(we, True, int_bits, TOTAL_BITS - int_bits))
                error += err * err
            for be in b:
                err = be - float(FixedPoint(be, True, int_bits, TOTAL_BITS - int_bits))
                error += err * err
            denom += w.shape[0] + b.shape[0]

    error /= denom
    rmse = torch.sqrt(error)

    print(f"INT_BITS = {int_bits} , RMSE = {rmse}")


# Output memory files for 32 perceptrons
