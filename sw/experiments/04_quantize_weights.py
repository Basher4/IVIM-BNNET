"""
This file loads parameters of NN generated by 01.py and
    1) Fuses batch normalization parameter with preceeding weights
    2) Finds the best nuber of integer and fractional bits to represent weights and biases
"""

from fixedpoint import FixedPoint
import itertools
import json
import numpy as np
import sys
import torch

sys.path.append('./sw/')

NEURAL_NETWORK_PATH = "./sw/models/bnn_e08_patient.pt"
MEM_FILE_PATH = "./mem_files/e08_patient"
ACCELERATOR_NUM_LANES = 32
accelerator_perceptrons = [[]] * ACCELERATOR_NUM_LANES
TOTAL_BITS = 16
MEMORY_WIDTH = 128 + 1

# Load neural network
net: torch.nn.Module = torch.load(NEURAL_NETWORK_PATH, map_location="cpu")
network_parameters = dict(net.named_parameters())

# Merge weights with Batch Normalization
def merge_weights_with_bn(lin_w, lin_b, bn_w, bn_b):
    w = lin_w * bn_w
    b = bn_w * lin_b + bn_b
    return w.detach(), b.detach()

param_sum, param_count = 0, 0
params_arr = []

for param_net in range(4):
    for offset in [0, 4]:
        linear_layer: torch.nn.Linear = net.get_submodule(f"fc_layers.{param_net}.{offset}")
        batch_norm: torch.nn.BatchNorm1d = net.get_submodule(f"fc_layers.{param_net}.{offset+2}")

        lin_w, lin_b = linear_layer.weight, linear_layer.bias
        bn_w, bn_b = batch_norm.weight, batch_norm.bias
        for perc_idx in range(lin_w.shape[0]):
            w, b = merge_weights_with_bn(lin_w[perc_idx], lin_b[perc_idx], bn_w[perc_idx], bn_b[perc_idx])
            accelerator_perceptrons[perc_idx % ACCELERATOR_NUM_LANES].append((w,b))
            params_arr.append(w.numpy())
            params_arr.append(b.numpy().reshape(1))

    encoder_layer: torch.nn.Linear = net.get_submodule(f"encoder.{param_net}.8")
    for i in range(ACCELERATOR_NUM_LANES):
        accelerator_perceptrons[i].append((encoder_layer.weight.squeeze(), encoder_layer.bias[0]))

params_arr = np.concatenate(params_arr)

# Find the best quantization
print(f"Every perceptron ({ACCELERATOR_NUM_LANES} in total) requires memory {len(accelerator_perceptrons[0])} deep")
print(f"Parameter stats:\n\tCount = {len(params_arr)}\n\tMean  = {np.mean(params_arr)}\n\tMedian= {np.median(params_arr)}\n\tStdev = {np.std(params_arr)}\n")

# best_int_bits = 0
# best_rmse = 9999
# for int_bits in range(1, TOTAL_BITS):
#     error = 0
#     rmse = 0
#     denom = 0

#     for w,b in itertools.chain.from_iterable(accelerator_perceptrons):
#         for we in w:
#             err = we - float(FixedPoint(we, True, int_bits, TOTAL_BITS - int_bits))
#             error += err * err

#         err = b - float(FixedPoint(b, True, int_bits, TOTAL_BITS - int_bits))
#         error += err * err

#         denom += w.shape[0] + 1

#     error /= denom
#     rmse = torch.sqrt(error)

#     print(f"INT_BITS = {int_bits} , RMSE = {rmse}")

#     if rmse < best_rmse:
#         best_rmse = rmse
#         best_int_bits = int_bits
#     else:
#         break
best_int_bits = 3

# Output memory files for 32 perceptrons
for neuron_idx, neuron_data in enumerate(accelerator_perceptrons):
    # Convert data into the correct shape.
    quant_perc_data = []
    for w,b in neuron_data:
        pdata = []
        pdata.append(FixedPoint(b, True, best_int_bits, TOTAL_BITS - best_int_bits))
        for wi, we in enumerate(w):
            pdata.append(FixedPoint(we, True, best_int_bits, TOTAL_BITS - best_int_bits))

        while len(pdata) < MEMORY_WIDTH:
            pdata.append(FixedPoint(0, True, best_int_bits, TOTAL_BITS - best_int_bits))

        quant_perc_data.append(pdata)
    
    # Output it to a mem file.
    with open(f"{MEM_FILE_PATH}/perc_{neuron_idx}_params.mem", "w") as fd:
        for row in quant_perc_data:
            for elem in row:
                fd.write(f"{elem:0{4}x}")
            fd.write('\n')
